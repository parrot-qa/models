{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parrot-qa/models/blob/main/DPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJAMy0P5XVtw"
      },
      "source": [
        "# Pre-requisites\n",
        "\n",
        "**Need to Have:** The dataset JSON file `parrot-qa.json` generated using the 'parrot-qa/dataset' repository.\n",
        "\n",
        "Upload it to a `data` directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLp2fkiwJouz"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "\n",
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install datasets\n",
        "!pip install nltk rouge_score\n",
        "\n",
        "#!pip install farm-haystack[colab,faiss]\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab,faiss]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlZgP8q1A6NW"
      },
      "outputs": [],
      "source": [
        "# Make sure you have a GPU running\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEH-CRbeA6NU"
      },
      "source": [
        "# Step 1: Dense Passage Retrieval\n",
        "\n",
        "We will use the DPR model introduced by Karpukhin et al. (2020, https://arxiv.org/abs/2004.04906). \n",
        "\n",
        "Original Code: https://fburl.com/qa-dpr\n",
        "\n",
        "The original reference notebook is [here](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxUz5x40Is_V"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "# Split documents into pieces, the module respects sentence boundaries.\n",
        "PREPROC_SPLIT_LEN_DOC = 100\n",
        "ANSWER_PREFERENCE = 'instructor'  # 'instructor' or 'student'\n",
        "\n",
        "# Retriever parameters\n",
        "MAX_SEQ_LEN_QUERY = 256\n",
        "MAX_SEQ_LEN_PASSAGE = 128\n",
        "RETRIEVER_BATCH_SIZE = 16\n",
        "\n",
        "RETRIEVER_TOP_K = 5\n",
        "READER_TOP_K = 5\n",
        "USE_CONTEXT_FROM = 'retriever'  # 'retriever' or 'reader'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06LatTJBA6N0",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Cleaning & Indexing\n",
        "\n",
        "We group documents by course and index them into the DocumentStore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHuYZ2cEiHeT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "from haystack.nodes import PreProcessor\n",
        "\n",
        "\n",
        "def _format_title(title):\n",
        "    title = ' '.join(re.findall(r'[a-z0-9.-]+', title, re.IGNORECASE))\n",
        "    return title\n",
        "\n",
        "\n",
        "def _get_answer(answers):\n",
        "    max_val = max(answers['score'])\n",
        "    if ANSWER_PREFERENCE == 'student':\n",
        "        max_idx = answers['score'].index(max_val)\n",
        "    else:\n",
        "        # Instructor answer is stored last\n",
        "        max_idx = answers['score'][::-1].index(max_val)\n",
        "        max_idx = len(answers['score']) - 1 - max_idx\n",
        "    return answers['text'][max_idx]\n",
        "\n",
        "\n",
        "def extract_docs(dataset):\n",
        "    # Store one list of documents per course\n",
        "    docs_db = {}\n",
        "\n",
        "    for doc in dataset['documents']:\n",
        "        course = doc['course']\n",
        "        if course not in docs_db:\n",
        "            docs_db[course] = []\n",
        "        docs_db[course].append({\n",
        "            'content': doc['passage_text'],\n",
        "            'meta': {'name': _format_title(doc['article_title'])},\n",
        "        })\n",
        "\n",
        "    preproc = PreProcessor(split_length=PREPROC_SPLIT_LEN_DOC)\n",
        "    for course in docs_db.keys():\n",
        "        docs_db[course] = preproc.process(docs_db[course])\n",
        "\n",
        "    # It seems preproc sometimes ends up with duplicate IDs, so cleanup manually\n",
        "    for course, docs in docs_db.items():\n",
        "        for idx, doc in enumerate(docs):\n",
        "            doc.id = f'd{idx}'\n",
        "\n",
        "    return docs_db\n",
        "\n",
        "\n",
        "def extract_qa_pairs(dataset):\n",
        "    # Store one list of documents per course\n",
        "    qa_db = {}\n",
        "\n",
        "    for qa in dataset['qa_pairs']:\n",
        "        course = qa['course']\n",
        "        if course not in qa_db:\n",
        "            qa_db[course] = []\n",
        "        if qa['is_answerable'] == False:\n",
        "            continue\n",
        "        qa_db[course].append({\n",
        "            'question': qa['title'],\n",
        "            'answer': _get_answer(qa['answers'])})\n",
        "\n",
        "    return qa_db\n",
        "\n",
        "\n",
        "with open(\"data/parrot-qa.json\") as file_path:\n",
        "    dataset = json.load(file_path)\n",
        "\n",
        "docs_db = extract_docs(dataset)\n",
        "qa_db = extract_qa_pairs(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3dSo7ZtA6Nl"
      },
      "source": [
        "### Document Store & Retriever\n",
        "\n",
        "#### FAISS\n",
        "\n",
        "FAISS is a library for efficient similarity search on a cluster of dense vectors.\n",
        "The `FAISSDocumentStore` uses a SQL(SQLite in-memory be default) database under-the-hood\n",
        "to store the document text and other meta data. The vector embeddings of the text are\n",
        "indexed on a FAISS Index that later is queried for searching answers.\n",
        "The default flavour of FAISSDocumentStore is \"Flat\" but can also be set to \"HNSW\" for\n",
        "faster search at the expense of some accuracy. Just set the faiss_index_factor_str argument in the constructor.\n",
        "For more info on which suits your use case: https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n",
        "\n",
        "#### Retriever\n",
        "\n",
        "**Here:** We use a `DensePassageRetriever`\n",
        "\n",
        "**Alternatives:**\n",
        "\n",
        "- The `ElasticsearchRetriever`with custom queries (e.g. boosting) and filters\n",
        "- Use `EmbeddingRetriever` to find candidate documents based on the similarity of embeddings (e.g. created via Sentence-BERT)\n",
        "- Use `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmRuhTQ7A6Nh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from haystack.nodes import DensePassageRetriever\n",
        "from haystack.document_stores import FAISSDocumentStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqKnu6wxA6N1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# For each course, embed the pool of documents and create retrievers\n",
        "\n",
        "dpr_db = {}\n",
        "\n",
        "for course, docs in docs_db.items():\n",
        "    db_file = f'data/faiss_document_store_{course}.db'\n",
        "    if os.path.isfile(db_file):\n",
        "        os.remove(db_file)\n",
        "    document_store = FAISSDocumentStore(\n",
        "        sql_url=f\"sqlite:///{db_file}\",\n",
        "        faiss_index_factory_str=\"Flat\",\n",
        "    )\n",
        "    document_store.write_documents(docs, duplicate_documents='fail')\n",
        "\n",
        "    retriever = DensePassageRetriever(\n",
        "        document_store=document_store,\n",
        "        query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "        passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
        "        max_seq_len_query=MAX_SEQ_LEN_QUERY,\n",
        "        max_seq_len_passage=MAX_SEQ_LEN_PASSAGE,\n",
        "        batch_size=RETRIEVER_BATCH_SIZE,\n",
        "        use_gpu=True,\n",
        "        embed_title=True,\n",
        "    )\n",
        "    document_store.update_embeddings(retriever)\n",
        "\n",
        "    dpr_db[course] = retriever\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVR28OXA6OA"
      },
      "source": [
        "### Reader\n",
        "\n",
        "Here we use a FARMReader with the *deepset/roberta-base-squad2* model (see: https://huggingface.co/deepset/roberta-base-squad2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyIuWVwhA6OB"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import FARMReader\n",
        "\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unhLD18yA6OF"
      },
      "source": [
        "### Pipeline\n",
        "\n",
        "With a Haystack `Pipeline` you can stick together your building blocks to a search pipeline.\n",
        "Under the hood, `Pipelines` are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases.\n",
        "To speed things up, Haystack also comes with a few predefined Pipelines. One of them is the `ExtractiveQAPipeline` that combines a retriever and a reader to answer our questions.\n",
        "You can learn more about `Pipelines` in the [docs](https://haystack.deepset.ai/docs/latest/pipelinesmd)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ2e1KwFQNyN"
      },
      "outputs": [],
      "source": [
        "from haystack.pipelines import Pipeline, ExtractiveQAPipeline\n",
        "\n",
        "\n",
        "def attach_context_retriever(qa_db, dpr_db):\n",
        "    for course, pairs in qa_db.items():\n",
        "        pipe = Pipeline()\n",
        "        pipe.add_node(component=dpr_db[course], name='Retriever', inputs=['Query'])\n",
        "        for qa in pairs:\n",
        "            context = pipe.run(\n",
        "                query=qa['question'],\n",
        "                params={\"Retriever\": {\"top_k\": RETRIEVER_TOP_K}}\n",
        "            )\n",
        "            qa['contexts'] = [doc.content for doc in context['documents']]\n",
        "\n",
        "\n",
        "def attach_context_reader(qa_db, dpr_db):\n",
        "    for course, pairs in qa_db.items():\n",
        "        pipe = ExtractiveQAPipeline(retriever=dpr_db[course], reader=reader)\n",
        "        for qa in pairs:\n",
        "            prediction = pipe.run(\n",
        "                query=qa['question'],\n",
        "                params={\"Retriever\": {\"top_k\": RETRIEVER_TOP_K}, \"Reader\": {\"top_k\": READER_TOP_K}}\n",
        "            )\n",
        "            qa['contexts'] = [ans.context for ans in prediction['answers']]\n",
        "\n",
        "\n",
        "if USE_CONTEXT_FROM == 'retriever':\n",
        "    attach_context_retriever(qa_db, dpr_db)\n",
        "elif USE_CONTEXT_FROM == 'reader':\n",
        "    attach_context_reader(qa_db, dpr_db)\n",
        "else:\n",
        "    raise RuntimeError('Invalid configuration for selecting context.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g9YOl7xAKC6"
      },
      "outputs": [],
      "source": [
        "# Statistics\n",
        "\n",
        "qlengths = []\n",
        "clengths = []\n",
        "alengths = []\n",
        "for course, pairs in qa_db.items():\n",
        "    for qa in pairs:\n",
        "        qlengths.append(len(qa['question']))\n",
        "        alengths.append(len(qa['answer']))\n",
        "        clengths.append(\n",
        "            sum(len(context) for context in qa['contexts'])\n",
        "        )\n",
        "\n",
        "print('Average question length (characters):', round(sum(qlengths) / len(qlengths)))\n",
        "print('Average context length (characters):', round(sum(clengths) / len(clengths)))\n",
        "print('Average answer length (characters):', round(sum(alengths) / len(alengths)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUdYaALW43t-"
      },
      "source": [
        "### Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LRKdqyn43t-"
      },
      "outputs": [],
      "source": [
        "# Write contextualized QA pairs to JSON\n",
        "\n",
        "qa_export = []\n",
        "for course, pairs in qa_db.items():\n",
        "    for qa in pairs:\n",
        "        item = {'course': course}\n",
        "        item.update(qa)\n",
        "        qa_export.append(item)\n",
        "\n",
        "with open('data/parrot-qa-ctx.json', 'w') as file_path:\n",
        "    json.dump(qa_export, file_path, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "random.shuffle(qa_export)\n",
        "\n",
        "N = len(qa_export)\n",
        "train_len = int(0.8 * N)\n",
        "dev_len = int(0.1 * N)\n",
        "test_len = N - train_len - dev_len\n",
        "\n",
        "with open('data/parrot-qa-ctx-train.json', 'w') as file_path:\n",
        "    json.dump(qa_export[:train_len], file_path, indent=4)\n",
        "\n",
        "with open('data/parrot-qa-ctx-dev.json', 'w') as file_path:\n",
        "    json.dump(qa_export[train_len:train_len+dev_len], file_path, indent=4)\n",
        "\n",
        "with open('data/parrot-qa-ctx-test.json', 'w') as file_path:\n",
        "    json.dump(qa_export[train_len+dev_len:], file_path, indent=4)\n"
      ],
      "metadata": {
        "id": "_l-xpZMP5ll8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ParrotQA_DPR_UnifiedQA.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}