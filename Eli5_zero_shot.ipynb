{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-bhBwXaDP6e"
      },
      "outputs": [],
      "source": [
        "!pip install nlp\n",
        "!pip install transformers\n",
        "!pip install rouge_score\n",
        "!pip install faiss_gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nlp\n",
        "from lfqa_utils import *\n",
        "import json"
      ],
      "metadata": {
        "id": "hx2HLNi5BooR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLLOj8yeDP6f"
      },
      "source": [
        "<a id='reddit_biases'></a>\n",
        "### 1.b - Note on Data and Biases\n",
        "\n",
        "Before we go any further, let us take a moment to talk about the provenance of our training data. While Reddit hosts a number of thriving communities with high quality discussions, it is also widely known to have corners where sexism, hate, and harassment are significant issues. See for example the [recent post from Reddit founder u/spez](https://www.reddit.com/r/announcements/comments/gxas21/upcoming_changes_to_our_content_policy_our_board/) outlining some of the ways he thinks the website's historical policies have been responsible for this problem, [Adrienne Massanari's 2015 article on GamerGate](https://www.researchgate.net/publication/283848479_Gamergate_and_The_Fappening_How_Reddit's_algorithm_governance_and_culture_support_toxic_technocultures) and follow-up works, or a [2019 Wired article on misogyny on Reddit](https://www.wired.com/story/misogyny-reddit-research/).\n",
        "\n",
        "While there has been some recent work in the NLP community on *de-biasing* models (e.g. [Black is to Criminal as Caucasian is to Police:\n",
        "Detecting and Removing Multiclass Bias in Word Embeddings](https://arxiv.org/abs/1904.04047) for word embeddings trained specifically on Reddit data), this problem is far from solved, and the likelihood that a trained model might learn the biases present in the data remains a significant concern.\n",
        "\n",
        "As mentioned above, the magnitude of the problem depends on the specific communities/subreddits. This work uses data from [r/explainlikeimfive](https://www.reddit.com/r/explainlikeimfive/), and the `nlp` library also gives access to examples from [r/askscience](https://www.reddit.com/r/askscience/), and [r/AskHistorians](https://www.reddit.com/r/AskHistorians/). There are some encouraging signs for all of these communities: [r/explainlikeimfive](https://www.reddit.com/r/explainlikeimfive/) and [r/askscience](https://www.reddit.com/r/askscience/) have similar structures and purposes, and [r/askscience](https://www.reddit.com/r/askscience/) was found in 2015 to show medium supportiveness and very low toxicity when compared to other subreddits (see a [hackerfall post](https://hackerfall.com/story/study-and-interactive-visualization-of-toxicity-in), [thecut.com write-up](https://www.thecut.com/2015/03/interactive-chart-of-reddits-toxicity.html) and supporting [data](https://chart-studio.plotly.com/~bsbell21/210/toxicity-vs-supportiveness-by-subreddit/#data)). Meanwhile, the [r/AskHistorians rules](https://www.reddit.com/r/AskHistorians/wiki/rules) mention that the admins will not tolerate \"*racism, sexism, or any other forms of bigotry*\".\n",
        "\n",
        "This is obviously not enough to exonerate the model (the pre-training step, for example, raises its own questions on that topic), and there is still a lot of interesting work to do to be able to quantify the biases in a conditional text generation model. One thing you can do to help: if you find any particularly egregious answers provided by the model when using the demo, or want to work on this research question please send a DM to [@YJernite on Twitter](https://twitter.com/YJernite)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lztrTfCgDP6g"
      },
      "source": [
        "<a id='task_description'></a>\n",
        "# 2. Task and Data Description\n",
        "\n",
        "Let's recap: we are interested in the task of Long Form Question Answering. As in other Question Answering tasks, the model is presented with a question, and is required to generate a natural language answer. Whereas a majority of QA datasets contain mostly **factoid** questions, where the answer, such as a date or the name of a single entity, can be expressed in a few words or single sentence, Long Form QA focuses on questions which call for an **explanation** consisting of a few sentences or a few paragraphs.\n",
        "\n",
        "In order to teach a model to answer such questions, we use questions and answers written by Reddit users. Note that the `nlp.load_dataset` command above actually downloaded questions and their associated answers from the [r/explainlikeimfive](https://www.reddit.com/r/explainlikeimfive/), [r/askscience](https://www.reddit.com/r/askscience/), and [r/AskHistorians](https://www.reddit.com/r/AskHistorians/) subreddits. We focus here on the **ELI5/explainlikeimfive** part to train the system, as these examples tend to be a little simpler.  \n",
        "\n",
        "Let's look at one item from the test set:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parrot = nlp.load_dataset('json',data_files='parrot-qa-train.json', field='data')['train']\n",
        "parrot_dev = nlp.load_dataset('json',data_files='parrot-qa-dev.json', field='data')['train']\n",
        "parrot_test = nlp.load_dataset('json',data_files='parrot-qa2.json', field='qa_pairs')['train']\n",
        "chunks = nlp.load_dataset('json',data_files='parrot-qa.json', field=\"documents\")['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bHLEk8lQBB4",
        "outputId": "1bb82cd9-d395-441d-fb69-76931032e6f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default\n",
            "Using custom data configuration default\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7jf9K5XDP6h"
      },
      "source": [
        "So here we have the question:\n",
        "> Why does water heated to room temperature feel colder than the air around it?  \n",
        "\n",
        "\n",
        "This definitely requires a multi-step explanation: no single phrase can sum up all of the information we are looking for. Here are the answers that were given on ELI5, and were given scores of +5 and +2 respectively by Reddit users:\n",
        "> 1. Water transfers heat more efficiently than air. When something feels cold it's because heat is being transferred from your skin to whatever you're touching. Since water absorbs the heat more readily than air, it feels colder.  \n",
        "\n",
        "> 2. Air isn't as good at transferring heat compared to something like water or steel (sit on a room temperature steel bench vs. a room temperature wooden bench, and the steel one will feel more cold). When you feel cold, what you're feeling is heat being transferred out of you. If there is no breeze, you feel a certain way.  If there's a breeze, you will get colder faster (because the moving air is pulling the heat away from you), and if you get into water, its quite good at pulling heat from you. Get out of the water and have a breeze blow on you while you're wet, all of the water starts evaporating, pulling even more heat from you.  \n",
        "\n",
        "First, note that in this case **we have two answers** which broadly describe the same phenomenon: the first one is scored higher because it is more succint and to the point. This example already illustrates one important feature of the LFQA task: **there are usually several valid ways to answer a given question.** Of the 272K examples in the ELI5 training set, nearly two thirds (167K) have at least two answers. We'll need to keep this in mind when training and evaluation of the model.  \n",
        "\n",
        "Secondly, we need to give our model access to the information that is expressed in both these answers. Recently released models have been shown to include a significant amount of world knowledge in their parameters without the need of any external knowledge at all (see e.g. the [Closed-book QA performance of the T5 model](https://arxiv.org/abs/2002.08910)). There are several advantages to giving the model explicit access to information in text form however. First, a larger number of parameters in a model implies a larger computational cost. Secondly, getting information from a text database allows us to easily update the model's knowledge without having to re-train its parameters.\n",
        "\n",
        "--- \n",
        "\n",
        "<img src=\"images/ELI5animation.gif\" width=\"750\" align=\"center\"/>  \n",
        "\n",
        "---\n",
        "\n",
        "<center> Overview of the full question answering process.</center>\n",
        "<center> First, the Document Retriever selects a set of passages from Wikipedia that have information relevant to the question.</center>\n",
        "<center> Then, the Answer Generation Model reads the concatenation of the question and retrieverd passages, and writes out the answer.</center>\n",
        "\n",
        "---  \n",
        "Here, we choose to give the model access to Wikipedia text. Full Wikipedia articles are typically too long for most current models to handle, and notable exceptions like the [Reformer](https://arxiv.org/abs/2001.04451) or [Longformer](https://arxiv.org/abs/2004.05150) architectures unfortunately do not yet have pre-trained sequence-to-sequence variants. Thus, we follow previous work in splitting Wikipedia articles into disjoint snippets of 100 words, and keep track of the title of the article and sections a snippet came from. Here's how you can get a pre-processed Wiki40b version split into 100-word passages with the `nlp` library, and an example snippet which has some of the information we're looking for (\"*little conduction would occur since air is a poor conductor of heat*\"):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgN2RxJzDP6h"
      },
      "source": [
        "In the next two sections, we show how we can use either a [sparse retriever](#elasticsearch) or a [trained dense retriever](#dense_retrieval) to automatically find relevant snippets for a question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXY7amoADP6i"
      },
      "source": [
        "<a id='dense_retrieval'></a>\n",
        "# 4. Retrieving Support Documents with an ELI5-Trained Dense Model\n",
        "\n",
        "The sparse retriever works by finding passages which feature the words from the query. However, it has no way to know *a priori* which of these words are more important in context, and seems to struggle with understanding the central theme of the query (human-perceived temperature).\n",
        "\n",
        "Thankfully, some recent works have taken advantage of advances in pre-trained contextual word representations to solve this problem. Models such as [DPR](https://arxiv.org/abs/2004.04906) or [REALM](https://arxiv.org/abs/2002.08909) for example learn to compute a vector representation of the query, as well as vector representations of Wikipedia passages in such a way that the passages that best answers a question maximize the dot product between the two representations. Retrieval is then reduced to a Maximum Inner Product Search, which can be executed efficiently using systems like [FAISS](https://github.com/facebookresearch/faiss).\n",
        "\n",
        "These successes are very encouraging for our Open-Domain Long Form QA application. However, our task and setup do not quite meet the requirements of either of either of these approaches. On the one hand, the [DPR](https://arxiv.org/abs/2004.04906) system is trained using gold passage annotations: most major QA dataset tell the system which Wikipedia passage contains the answer. Unfortunately, we do not have such annotations for the ELI5 data. On the other hand, while [REALM](https://arxiv.org/abs/2002.08909) is trained without passage supervision, it requires a pretty expensive pre-training step with an [Inverse Cloze Task](https://arxiv.org/abs/1906.00300) (100,000 steps with batch size 4096), and the ability to re-compute the embeddings of all Wikipedia passages regularly during training.\n",
        "\n",
        "In order to train a similar dense retrieval system at reduced cost without having access to gold passage annotation, we will have to **take advantage of another unique feature of our dataset**, namely the fact that the long form answers are quite similar in style to the Wikipedia passages we want to index. Our hypothesis then is that if we train a system to embed the questions and answers in our dataset in a way that allows us to easily match questions to answers, then using the answer embedder on Wikipedia passages should allow us to similarly match questions to supporting evidence from Wikipedia.\n",
        "\n",
        "<a id='dense_train'></a>\n",
        "### 4.a - Contrastive Training with ELI5 In-Batch Negatives\n",
        "\n",
        "As mentioned above, we want to train a system to produce question and answer embeddings, such that the dot product between the representation of a question and any of its answers is greater than between it and answers of all of the other questions in the dataset.  \n",
        "\n",
        "Unfortunately, actually comparing all questions to all answers before taking every single gradient step is computationally prohibitive: instead, we follow previous work in simply processing medium to large batches of question-answer pairs, and making sure that the dot product of a question with its answer is larger than with all other answers in the batch, and *vice versa*.  \n",
        "\n",
        "We use a cross-entropy loss for the multinomial distribution over all of the answers (or questions) in a batch, and make use of [PyTorch gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) to be able to use large batches with limited GPU memory: you can find all implementation details in the `RetrievalQAEmbedder` class in `eli5_utils.py`.\n",
        "\n",
        "---  \n",
        "\n",
        "<img src=\"images/ELI5contrastive.svg\" width=\"700\" align=\"center\"/>  \n",
        "\n",
        "---  \n",
        "\n",
        "<center> To train the retriever, we show the model batches of 512 question-answer pairs.</center>\n",
        "<center> The model needs to ensure that the embedding of each question in the batch is closer to the embedding</center>\n",
        "<center> of its corresponding answer than to the embedding of any other answer in the batch.</center>\n",
        "\n",
        "---  \n",
        "\n",
        "\n",
        "We use a single BERT-style pre-trained model to embed the questions and answers, and learn different projection matrices to bring both representations down to dimension 128: the projection matrices are trained from scratch as the sentence embedding model is fine-tuned. We found that the 8-layer distilled version of BERT from the [Well-Read Students Learn Better paper](https://arxiv.org/abs/1908.08962) performed as well or better as full BERT for a notable gain in computation speed: if you want an even faster model, that work provides pre-trained models spanning the full range of computation/accuracy trade-offs.\n",
        "\n",
        "The model can than be trained with the following code: with batch size 32/512 on a single 16GB GPU, you can run 10 training epochs in under 6 hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiDRCxFsDP6i"
      },
      "outputs": [],
      "source": [
        "# training arguments\n",
        "\n",
        "class ArgumentsQAR():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 512\n",
        "        self.max_length = 128\n",
        "        self.checkpoint_batch_size = 32\n",
        "        self.print_freq = 100\n",
        "        self.pretrained_model_name = \"google/bert_uncased_L-8_H-768_A-12\"\n",
        "        self.model_save_name = \"retriever_models/eli5_retriever_model_l-8_h-768_b-512-512\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs = 10\n",
        "\n",
        "qar_args = ArgumentsQAR()\n",
        "\n",
        "# prepare torch Dataset objects\n",
        "qar_train_dset = ELI5DatasetQARetriver(parrot, training=True)\n",
        "qar_valid_dset = ELI5DatasetQARetriver(parrot_dev, training=False)\n",
        "\n",
        "# load pre-trained BERT and make model\n",
        "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
        "        model_name=qar_args.pretrained_model_name,\n",
        "        from_file=None#,\n",
        "        #device=\"cuda:0\"\n",
        ")\n",
        "\n",
        "# train the model\n",
        "train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyGPfVw9DP6j"
      },
      "source": [
        "Once the model is trained, it can be used to compute passage embeddings for all Wikipedia snippets. The `make_qa_dense_index` method takes advantage of `numpy` memory-mapping, so embeddings are written directly to disk. Again with a single GPU, computing the full set of passage embeddings should take about 18 hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0nKUYqXQDP6j"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile('wiki40b_passages_reps_32_l-8_h-768_b-512-512.dat'):\n",
        "    make_qa_dense_index(\n",
        "        qar_model, qar_tokenizer, chunks, #device='cuda:0',\n",
        "        index_name='wiki40b_passages_reps_32_l-8_h-768_b-512-512.dat'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRuJxyGJDP6j"
      },
      "source": [
        "<a id='dense_use'></a>\n",
        "### 4.b -  Using the Trained Dense Retriever and Wikipedia Index\n",
        "\n",
        "Now that we have trained our model to compute query and answer embeddings and used it to compute passage embeddings for all our Wikipedia snippets, let's see whether it can actually find supporting evidence for a new question. Recall the the two steps to using the dense retriever: we first compute an embedding for a new question, then do Max Inner Product Search with the pre-computed passage representations.\n",
        "\n",
        "---  \n",
        "\n",
        "<img src=\"images/ELI5wiki_index.svg\" width=\"600\" align=\"center\"/>  \n",
        "\n",
        "---  \n",
        "\n",
        "<center> At test time, the Retriever Model encodes the question, and compares its embedding to the pre-computed representation of</center>\n",
        "<center>  all the Wikipedia passages. The ten passages with the closest embedding are returned to create the support document.</center>\n",
        "\n",
        "---  \n",
        "\n",
        "The MIPS part can be executed efficiently with the `faiss` library. Additionally, since we computed 128-dimensional passage embeddings, the whole of the representations fits on a GPU, making retrieval even faster. We can create the `faiss_gpu` index with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RUO8EIr8DP6j"
      },
      "outputs": [],
      "source": [
        "faiss_res = faiss.StandardGpuResources()\n",
        "wiki40b_passage_reps = np.memmap(\n",
        "            'wiki40b_passages_reps_32_l-8_h-768_b-512-512.dat',\n",
        "            dtype='float32', mode='r+',\n",
        "            shape=(chunks.num_rows, 128)\n",
        ")\n",
        "\n",
        "wiki40b_index_flat = faiss.IndexFlatIP(128)\n",
        "wiki40b_gpu_index = faiss.index_cpu_to_gpu(faiss_res, 0, wiki40b_index_flat)\n",
        "wiki40b_gpu_index.add(wiki40b_passage_reps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPrBEVK-DP6j"
      },
      "source": [
        "Now we can use the `query_qa_dense_index` function to query the dense index for our running example question about perceived temperature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "JK0bAH3bDP6j",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08094dbe-8228-4746-975b-ec4cd37fb098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fa8ba8caf50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_08709_row0_col0, #T_08709_row0_col1, #T_08709_row0_col2, #T_08709_row1_col0, #T_08709_row1_col1, #T_08709_row1_col2, #T_08709_row2_col0, #T_08709_row2_col1, #T_08709_row2_col2, #T_08709_row3_col0, #T_08709_row3_col1, #T_08709_row3_col2, #T_08709_row4_col0, #T_08709_row4_col1, #T_08709_row4_col2, #T_08709_row5_col0, #T_08709_row5_col1, #T_08709_row5_col2, #T_08709_row6_col0, #T_08709_row6_col1, #T_08709_row6_col2, #T_08709_row7_col0, #T_08709_row7_col1, #T_08709_row7_col2, #T_08709_row8_col0, #T_08709_row8_col1, #T_08709_row8_col2, #T_08709_row9_col0, #T_08709_row9_col1, #T_08709_row9_col2, #T_08709_row10_col0, #T_08709_row10_col1, #T_08709_row10_col2 {\n",
              "  text-align: left;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_08709_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Article</th>\n",
              "      <th class=\"col_heading level0 col1\" >Sections</th>\n",
              "      <th class=\"col_heading level0 col2\" >Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_08709_row0_col0\" class=\"data row0 col0\" >---</td>\n",
              "      <td id=\"T_08709_row0_col1\" class=\"data row0 col1\" >---</td>\n",
              "      <td id=\"T_08709_row0_col2\" class=\"data row0 col2\" >--- Error: Could not import scheme HI there, I keep getting this error. I have quit and re-opened VS code as well as re-downloaded scheme completely. Do you know how to fix it?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_08709_row1_col0\" class=\"data row1 col0\" >Syllabus & Course Policies: Assignments</td>\n",
              "      <td id=\"T_08709_row1_col1\" class=\"data row1 col1\" >Syllabus & Course Policies: Assignments</td>\n",
              "      <td id=\"T_08709_row1_col2\" class=\"data row1 col2\" >Each week, there will be problems assigned for you to work on, most of which will involve writing and analyzing programs. These assignments come in three categories: lab exercises, homework assignments, and projects.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_08709_row2_col0\" class=\"data row2 col0\" >Syllabus & Course Policies: Course Format: Exam Prep</td>\n",
              "      <td id=\"T_08709_row2_col1\" class=\"data row2 col1\" >Syllabus & Course Policies: Course Format: Exam Prep</td>\n",
              "      <td id=\"T_08709_row2_col2\" class=\"data row2 col2\" >Exam prep sessions will be held every Friday from 9:30am to 11am starting 1/28. The goal will be to recap the past week's material by way of going over past exam problems related to that material, as well as exploring some test-taking strategies. The problem walkthroughs will be recorded, so if you can't attend live, you'll still be able to watch the walkthroughs on your own.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_08709_row3_col0\" class=\"data row3 col0\" >Syllabus & Course Policies: Overview: Preparatory Classes: CS 10</td>\n",
              "      <td id=\"T_08709_row3_col1\" class=\"data row3 col1\" >Syllabus & Course Policies: Overview: Preparatory Classes: CS 10</td>\n",
              "      <td id=\"T_08709_row3_col2\" class=\"data row3 col2\" >CS 10: The Beauty and Joy of Computing is an introductory computer science course which is similar to CS 61A but moves at a friendlier pace. CS 10 covers variables, functions, recursion, algorithmic complexity, object-oriented programming, and many other relevant CS 61A topics, with the overall content overlap being about 50%. CS 10 starts the semester in Snap!, a block-based programming language which allows students to focus on conceptual understanding without worrying about unfamiliar syntax. After the midterm, the course transitions into Python (the primary language 61A uses), applying the same concepts you already learned to the new language, as well as introducing new concepts more relevant to Python. CS 10 also covers big ideas and social implications that go beyond programming, showing you the beauty and joy of computing.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_08709_row4_col0\" class=\"data row4 col0\" >Syllabus & Course Policies: Grading</td>\n",
              "      <td id=\"T_08709_row4_col1\" class=\"data row4 col1\" >Syllabus & Course Policies: Grading</td>\n",
              "      <td id=\"T_08709_row4_col2\" class=\"data row4 col2\" >Your course grade is computed using a point system with a total of 300 points, broken down as follows:  • Midterm 1, worth 40 points • Midterm 2, worth 50 points • Final Exam, worth 75 points • Projects, worth 99 points • Homework, worth 16 points • Lab, worth 10 points • Lab Participation, worth 5 points • Discussion Participation, worth 5 points  There are a handful extra credit points throughout the semester, perhaps around 10, that are available to everyone. Each letter grade for the course corresponds to a range of scores: Your final score will be rounded to the nearest integer before being converted to a letter grade. 0.5 rounds up to 1, but 0.49 rounds down to 0. There is no curve; your grade will depend only on how well you do, and not on how well everyone else does. Score thresholds are based on how students performed in previous semesters. Unlike some previous semesters you may have heard about, these thresholds will not be adjusted based on student performance. You could all get A's. You could all get D's. These are the exact thresholds that will be used at the end of the course to assign grades. In a typical semester, about 60% of students taking the course for a letter grade will receive a B+ or higher. Incomplete grades will be granted only for medical or personal emergencies that cause you to miss the final or last part of the course, only for students who have completed the majority of the coursework, and only if work up to the point of the emergency has been satisfactory. Your lowest homework score will be dropped. Each lab that you complete is worth 1 point, and you can receive a maximum of 10 lab points. There are going to be at least 12 lab assignments, so you can skip some and still get full credit.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_08709_row5_col0\" class=\"data row5 col0\" >Syllabus & Course Policies: Grading: Lab Participation</td>\n",
              "      <td id=\"T_08709_row5_col1\" class=\"data row5 col1\" >Syllabus & Course Policies: Grading: Lab Participation</td>\n",
              "      <td id=\"T_08709_row5_col2\" class=\"data row5 col2\" >The lab participation score is designed to make sure that all students attend at least the first few weeks of lab sections to try them out. Attending a lab will earn you one lab participation credit. There will be about 12 possible credits available. To earn a perfect lab participation score in the course, you need to earn at least 5 credits. Your course lab participation score is the number of lab participation credits you earn over the semester, up to 5. These are separate from the lab score component, which is graded based on lab completion and correctness.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_08709_row6_col0\" class=\"data row6 col0\" >Syllabus & Course Policies: Overview: Alternative Classes: CS 88</td>\n",
              "      <td id=\"T_08709_row6_col1\" class=\"data row6 col1\" >Syllabus & Course Policies: Overview: Alternative Classes: CS 88</td>\n",
              "      <td id=\"T_08709_row6_col2\" class=\"data row6 col2\" >CS 88: Computational Structures in Data Science is an introduction to programming and computing that has more than 50% concept overlap with CS 61A. It is designed for students interested in data science who want to expand their knowledge of programming and program structures beyond what is covered in Data 8. Students who complete CS 88 can either proceed directly to CS 61B or subsequently take CS 61A, a path that offers a substantial amount of review because of the high topic overlap between the courses.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_08709_row7_col0\" class=\"data row7 col0\" >Syllabus & Course Policies: Overview: Preparatory Classes: Data 8</td>\n",
              "      <td id=\"T_08709_row7_col1\" class=\"data row7 col1\" >Syllabus & Course Policies: Overview: Preparatory Classes: Data 8</td>\n",
              "      <td id=\"T_08709_row7_col2\" class=\"data row7 col2\" >Data 8: The Foundations of Data Science is an introduction to data science designed to be accessible and useful for all Berkeley students. This course was built for students without prior programming experience. It teaches students to program in Python 3, but covers a much smaller subset of the language than CS 61A. Most of the course focuses on data processing and statistical techniques that are central to using computers to answer questions about the world. Taking Data 8 before 61A is a good way to gain prior programming experience, but taking CS 10 is a better way.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_08709_row8_col0\" class=\"data row8 col0\" >Syllabus & Course Policies: Accommodations (DSP and Otherwise): Accommodations Appointments</td>\n",
              "      <td id=\"T_08709_row8_col1\" class=\"data row8 col1\" >Syllabus & Course Policies: Accommodations (DSP and Otherwise): Accommodations Appointments</td>\n",
              "      <td id=\"T_08709_row8_col2\" class=\"data row8 col2\" >If you're not enrolled in DSP, or are in the process of being onboarded by DSP, you may still be eligible for accommodations. You may also be eligible for accommodations if serious extenuating circumstances should come up during the semester. If you believe you may require accommodations, please visit this calendar to book a short (20-minute) appointment with our Student Support TA, Cooper Bedin. You can also reach them via email at at cooper.bedin@berkeley.edu.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_08709_row9_col0\" class=\"data row9 col0\" >Syllabus & Course Policies: Grading: Citizenship</td>\n",
              "      <td id=\"T_08709_row9_col1\" class=\"data row9 col1\" >Syllabus & Course Policies: Grading: Citizenship</td>\n",
              "      <td id=\"T_08709_row9_col2\" class=\"data row9 col2\" >For exceptionally rude or disrespectful behavior toward the course staff or other students, your final grade will be lowered by up to a full letter grade (e.g., from an A- to a B-) at the discretion of the course instructors. You don't need to be concerned about this policy if you treat other human beings with even a bare minimum of respect and consideration and do not engage in behavior that is actively harmful to others.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_08709_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_08709_row10_col0\" class=\"data row10 col0\" >Syllabus & Course Policies: Exams</td>\n",
              "      <td id=\"T_08709_row10_col1\" class=\"data row10 col1\" >Syllabus & Course Policies: Exams</td>\n",
              "      <td id=\"T_08709_row10_col2\" class=\"data row10 col2\" >There will be three exams:  • Midterm 1 will be held 8pm-10pm on Monday, 2/7. • Midterm 2 will be held 8pm-10pm on Thursday, 3/17. • The final exam will be held 11:30am-2:30pm on Tuesday, 5/10.  Exams will be taken on paper on campus in designated exam rooms. In the event that in-person exams are not allowed due to campus closure or room occupancy restriction, exams will be delivered remotely. You will be allowed to request a remote exam or other exam accommodations under specific circumstances. Details on how to request these for each exam will be released around 10-14 days prior to each exam. Unlike some courses, the particular subject matter of CS 61A makes it very difficult to ensure a fair exam in which many students are taking the exam in person and many others are taking it remotely. Since it is not likely that all on-campus students will have a suitable space to take a remote exam simultaneously, an in-person exam is the best available option, and this is why the default exam format is in-person. Students who are enrolled in another course with a conflicting final exam time may take the CS 61A final exam in the next exam slot: 3pm-6pm on Tuesday, 5/10. Proof of enrollment in a conflicting course will be required. This information will be collected around 2-3 weeks before the final exam.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "question = parrot_test[3]['title']\n",
        "doc, res_list = query_qa_dense_index(question, qar_model, qar_tokenizer, chunks, wiki40b_gpu_index, device='cuda:0')\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Article': ['---'] + [res['article_title'] for res in res_list],\n",
        "    'Sections': ['---'] + [res['section_title'] if res['section_title'].strip() != '' else res['article_title']\n",
        "                 for res in res_list],\n",
        "    'Text': ['--- ' + question] + [res[\"passage_text\"] for res in res_list],\n",
        "})\n",
        "df.style.set_properties(**{'text-align': 'left'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oJxtY6LDP6j"
      },
      "source": [
        "The retrieved documents are quite different from the ones returned by the sparse retrieval, with a greater focus on how water helps draw heat from a body, either through evaporation or through better conduction, which is information the model needs to answer this question.\n",
        "\n",
        "The retriever still misses out on one aspect of the query: the way the question is formulated implies that in the considered scenario the person is immersed in water rather than just wet, which makes the \"latent heat\" and evaporation arguments a little less relevant, but that's a really subtle distinction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8fmdoY1DP6j"
      },
      "source": [
        "<a id='dense_eval'></a>\n",
        "### 4.c -  Retriever Model Evaluation\n",
        "\n",
        "We have trained a retrieval model that *seems* to be working a little better than the traditional word-matching based approach, at least on our running example. Before we use it to actually answer questions, however, we would like to be able to get some **quantitative evaluation** of the performances of both approaches.\n",
        "\n",
        "For the retriever, we want to favor **recall** over precision: our first priority is to make sure that all of the information needed to write the answers is present in the support document. If there is unrelated information, the generation model can learn to sort it out. We measure this by computing the proportion of words in the high-scoring answers which are present in the retrieved support document. To focus on important words, we also weigh answer words by their *Inverse Document Frequency*. This gives us the following **IDF-recall** scoring function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "LFyK-zveDP6j"
      },
      "outputs": [],
      "source": [
        "# We first select high-scoring answers (answers beyond the first must have a score of at least 3)\n",
        "test_qa_list = [(exple['title'],\n",
        "                ' '.join([a \n",
        "                          for i, (a, sc) in enumerate(zip(exple['answers']['text'], exple['answers']['score'])) \\\n",
        "                          if i == 0 or sc >= 3\n",
        "                         ]))\n",
        "                for exple in parrot_test]\n",
        "\n",
        "# We then compute word frequencies in answer text\n",
        "answer_doc_freq = {}\n",
        "for q, a in test_qa_list:\n",
        "    for w in a.lower().split():\n",
        "        answer_doc_freq[w] = answer_doc_freq.get(w, 0) + 1\n",
        "\n",
        "# The IDF-recall function is then:\n",
        "def da_idf_recall(doc, answer):\n",
        "    d_words = dict([(w, True) for w in doc.lower().split()])\n",
        "    a_words = answer.lower().split()   \n",
        "    recall = sum([1. / math.log(1 + answer_doc_freq.get(w, 1)) for w in a_words if w in d_words]) / \\\n",
        "                sum([1. / math.log(1 + answer_doc_freq.get(w, 1)) for w in a_words])\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8frwZxjDP6j"
      },
      "source": [
        "The `evaluate_retriever` function in `eli5_utils.py` takes a retrieval and scoring function and computes both the average retrieval time and score of the document relative the the provided answer. Let's write some short-hand functions for the dense and sparse retrievers with our currently loaded indexes, and evaluate them on the ELI5 test set (be advised that evaluating the retriever on the full test set takes up to two hours):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "upu9P5ViDP6j",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "79c994b1-0777-4e9e-b006-b156c82188f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fa8b1072690>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_25c6a_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >IDF-Recall</th>\n",
              "      <th class=\"col_heading level0 col1\" >Time/Query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_25c6a_level0_row0\" class=\"row_heading level0 row0\" >Sparse</th>\n",
              "      <td id=\"T_25c6a_row0_col0\" class=\"data row0 col0\" >0.2394</td>\n",
              "      <td id=\"T_25c6a_row0_col1\" class=\"data row0 col1\" >0.0098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_25c6a_level0_row1\" class=\"row_heading level0 row1\" >Dense</th>\n",
              "      <td id=\"T_25c6a_row1_col0\" class=\"data row1 col0\" >0.2394</td>\n",
              "      <td id=\"T_25c6a_row1_col1\" class=\"data row1 col1\" >0.0098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "def dense_ret_for_eval(question, n_ret):\n",
        "    _, dense_res_list = query_qa_dense_index(\n",
        "        question, qar_model, qar_tokenizer, chunks, wiki40b_gpu_index, n_results=n_ret, device='cuda:0'\n",
        "    )\n",
        "    #print(dense_res_list)\n",
        "    dense_doc = ' '.join([res['passage_text'] for res in dense_res_list])\n",
        "    with open('dense_docs.json', 'w', encoding='utf-8') as f:\n",
        "      json.dump(dense_res_list, f, ensure_ascii=False, indent=4)\n",
        "    return dense_doc\n",
        "\n",
        "def sparse_ret_for_eval(question, n_ret):\n",
        "    _, sparse_res_list = query_es_index(\n",
        "        question, es_client, index_name='wiki40b_snippets_100w', n_results=n_ret\n",
        "    )\n",
        "    sparse_doc = ' '.join([res['passage_text'] for res in sparse_res_list])\n",
        "    return sparse_doc\n",
        "\n",
        "dense_score = evaluate_retriever(test_qa_list, dense_ret_for_eval, da_idf_recall)\n",
        "#sparse_score = evaluate_retriever(test_qa_list, sparse_ret_for_eval, da_idf_recall)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'IDF-Recall': [dense_score['idf_recall']],\n",
        "    'Time/Query': [dense_score['retrieval_time']],\n",
        "}, index=[ 'Sparse', 'Dense'])\n",
        "df.style.format({'IDF-Recall': \"{:.4f}\", 'Time/Query': \"{:.4f}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j046LTclDP6k"
      },
      "source": [
        "This metric obviously has limitations. Since it only looks at individual word matches, it is oblivious to *word order* or *paraphrases* among others. However, we can be encouraged by the fact that the dense retriever not only yields **higher IDF-recall**, it also takes **less than a third of the time** of the ElasticSearch-based system! Considering these results, we can confidently use it for the next part: training the sequence-to-sequence answer generation system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS5PJddiDP6k"
      },
      "source": [
        "<a id='generation'></a>\n",
        "# 5. Generating Answers with a Sequence-to-Sequence Model\n",
        "\n",
        "Now that we know how to create an evidence document with supporting information for a given question, let's look into training the second component of our system: the **answer generation module**. We will instantiate it as a sequence-to-sequence model which uses the [BART](https://arxiv.org/abs/1910.13461) architecture, and initialize it with the [bart-large pretrained weights](https://huggingface.co/facebook/bart-large).  \n",
        "\n",
        "In short, the [BART paper](https://arxiv.org/abs/1910.13461) uses a denoising auto-encoder style objective to pre-train an encoder-decoder model (similarly to how masked language modeling is used to pre-trained BERT-style encoders). Among other applications, they show that large-scale pre-training with their objective followed by fine-tuning on ELI5 data yields the state-of-the-art ROUGE performance for the original version of the dataset (which uses pre-computed support documents made from CommonCrawl pages).\n",
        "\n",
        "We provide the concatenation of the question and support document as input to the model, and train the decoder to minimize the perplexity of the gold answer. One notable choice is that **we train the model using all high-scoring answers in the training set**, so the model will see several instances of the same question-document input with different outputs. The supporting passages are separated by a special token `<P>`, so the input for our running example will look like:\n",
        "\n",
        "> question: Why does water heated to room temperature feel colder than the air around it? context: \\\\<P\\> when the skin is completely wet. The body continuously loses ... this heat comes from the liquid itself and the surrounding gas and surfaces. \\\\<P\\> protected by a glass panel. Consequently, these types of collectors... Since heat loss due to convection cannot cross a vacuum, it forms an efficient isolation mechanism to keep heat inside the collector pipes. Since two flat \\\\<P\\> ... \\\\<P\\> changes. Conduction On... Fluids—especially gases—are less conductive. Thermal contact conductance is the study of heat conduction between solid bodies in contact. The process of heat transfer\n",
        "\n",
        "The first thing we do is pre-compute the support documents for the training and validation sets so we can use all available GPUs to train the sequence-to-sequence model. The model is then trained with the `train_qa_s2s` function in `eli5_utils.py`. A 16GB GPU accomodates up to two examples at a time, so here is the code to train the model using 4 GPUs with `torch.nn.DataPArallel`. One epoch should take about 18 hours:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fWYMth4DP6k"
      },
      "source": [
        "Again, if you don't want to train the model yourself, we made trained weights available on the [Hugging Face model repository](https://huggingface.co/models) , which you can download with:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_s2s_tokenizer = AutoTokenizer.from_pretrained('yjernite/bart_eli5')\n",
        "qa_s2s_model = AutoModelForSeq2SeqLM.from_pretrained('yjernite/bart_eli5').to('cuda:0')\n",
        "_ = qa_s2s_model.eval()"
      ],
      "metadata": {
        "id": "o6j6B0nwNzL2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgCuL_ZBDP6k"
      },
      "source": [
        "We now have everything we need to answer any question! Now let's try the full system on our running example along with the first four questions of the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "sRBIO2F_DP6k",
        "outputId": "04fd3cc3-d28c-4215-cd47-aaec9db1128b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fa8b0921390>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a2c09_row0_col0, #T_a2c09_row0_col1, #T_a2c09_row1_col0, #T_a2c09_row1_col1, #T_a2c09_row2_col0, #T_a2c09_row2_col1, #T_a2c09_row3_col0, #T_a2c09_row3_col1, #T_a2c09_row4_col0, #T_a2c09_row4_col1 {\n",
              "  text-align: left;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a2c09_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Question</th>\n",
              "      <th class=\"col_heading level0 col1\" >Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a2c09_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a2c09_row0_col0\" class=\"data row0 col0\" >Midterm 2 Corrections Hello,\n",
              "I have some questions in regards to 2c. --> How do we know reversed is an iterable?\n",
              "For the seventh line in the doctest, why isn't None added after lst3 when extending? Correct if me if Im wrong, but doesnt extend return None?</td>\n",
              "      <td id=\"T_a2c09_row0_col1\" class=\"data row0 col1\" >I'm not sure what you mean by \"how do we know\"? It's not that we know, it's that we can prove it's true. If it's not true, then it can't be an iterable. An iterable is an operation that can be repeated over and over again. It can't just be repeated once, it has to be repeated multiple times.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a2c09_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_a2c09_row1_col0\" class=\"data row1 col0\" >Attendance Is there a way to check how many labs and discussions we have attended?</td>\n",
              "      <td id=\"T_a2c09_row1_col1\" class=\"data row1 col1\" >There is no such thing as a way to check how many labs and discussions you have attended. There is a way, however, to track how many classes you have taken and how many lectures you've attended. This is called the [ Attendance-to-Class Ratio]( URL_0 ), and it's used by colleges and universities to track the number of students who have taken a course.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a2c09_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_a2c09_row2_col0\" class=\"data row2 col0\" >Submission Is it okay that if my partner submitted both checkpoint1 and checkpoint2, can I submit extra credits this time?</td>\n",
              "      <td id=\"T_a2c09_row2_col1\" class=\"data row2 col1\" >No, you can't submit both at the same time. You can only submit one at a time, and if you submit both simultaneously, you'll get the same amount of credits as if you had only submitted checkpoint1 and checkpoint2. If you submit checkpoint1, checkpoint2, and checkpoint1 again, you will get the extra credits you submitted the first time.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a2c09_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_a2c09_row3_col0\" class=\"data row3 col0\" >Error on another device I have passed all tests on my MacBook, but I got a lot of errors when running the same code on a new device with windows system. Why could this happen?</td>\n",
              "      <td id=\"T_a2c09_row3_col1\" class=\"data row3 col1\" >It's possible that you're running the same code on a different device with a different operating system. It's also possible that the operating system on your new device is different than the one on your old device, or that the OS is different from the OS on the old device. It could also be that your operating system is different, or the OS in the new device isn't the same as the one in the old one.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a2c09_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_a2c09_row4_col0\" class=\"data row4 col0\" >Error: Could not import scheme HI there, I keep getting this error. I have quit and re-opened VS code as well as re-downloaded scheme completely. Do you know how to fix it?</td>\n",
              "      <td id=\"T_a2c09_row4_col1\" class=\"data row4 col1\" >I'm not sure what you mean by \"could not import scheme\", but it sounds like an error in the source code. If you want to know how to fix it, you'll have to go back to the original source code and look for the error. If it's not there, you can't fix it.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for i in [25] + [j for j in range(4)]:\n",
        "    # create support document with the dense index\n",
        "    question = parrot_test[i]['title']\n",
        "    doc, res_list = query_qa_dense_index(\n",
        "        question, qar_model, qar_tokenizer,\n",
        "        chunks, wiki40b_gpu_index, device='cuda:0'\n",
        "    )\n",
        "    # concatenate question and support document into BART input\n",
        "    question_doc = \"question: {} context: {}\".format(question, doc)\n",
        "    # generate an answer with beam search\n",
        "    answer = qa_s2s_generate(\n",
        "            question_doc, qa_s2s_model, qa_s2s_tokenizer,\n",
        "            num_answers=1,\n",
        "            num_beams=8,\n",
        "            min_len=64,\n",
        "            max_len=256,\n",
        "            max_input_length=1024,\n",
        "            device=\"cuda:0\"\n",
        "    )[0]\n",
        "    questions += [question]\n",
        "    answers += [answer]\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Question': questions,\n",
        "    'Answer': answers,\n",
        "})\n",
        "df.style.set_properties(**{'text-align': 'left'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJVKvgeTDP6k"
      },
      "source": [
        "<img src=\"images/fireworks.gif\" width=\"1000\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PMlhqXhDP6k"
      },
      "source": [
        "We made it, and a lot of these answers actually make sense! The model seems to sometimes struggle with coherence and with starting some of the answers, but we're getting some pretty good information overall.\n",
        "\n",
        "The last thing we'll do is see how we can get a quantitative evaluation of the model performance. Here, we'll use the ROUGE implementation provided in the `nlp` library.  \n",
        "\n",
        "Note that it is a different implementation than the one used in the [BART](https://arxiv.org/abs/1910.13461) and [ELI5](https://arxiv.org/abs/1907.09190) papers: the [rouge](https://pypi.org/project/rouge/) Python package they use normalises all numerical values, among other pre-processing choices, leading to higher numbers. We reproduce their evaluation in the Appendix section, but recommend using the more sensitive metric provided by the `nlp` package, which can be computed with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld2mGAhFDP6k"
      },
      "outputs": [],
      "source": [
        "predicted = []\n",
        "reference = []\n",
        "\n",
        "# Generate answers for the full test set\n",
        "for i in range(parrot_test.num_rows):\n",
        "    # create support document with the dense index\n",
        "    question = parrot_test[i]['title']\n",
        "    doc, res_list = query_qa_dense_index(\n",
        "        question, qar_model, qar_tokenizer,\n",
        "        chunks, wiki40b_gpu_index, device='cuda:0'\n",
        "    )\n",
        "    # concatenate question and support document into BART input\n",
        "    question_doc = \"question: {} context: {}\".format(question, doc)\n",
        "    # generate an answer with beam search\n",
        "    answer = qa_s2s_generate(\n",
        "            question_doc, qa_s2s_model, qa_s2s_tokenizer,\n",
        "            num_answers=1,\n",
        "            num_beams=8,\n",
        "            min_len=96,\n",
        "            max_len=256,\n",
        "            max_input_length=1024,\n",
        "            device=\"cuda:0\"\n",
        "    )[0]\n",
        "    predicted += [answer]\n",
        "    reference += [parrot_test[i]['answers']['text'][0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "pmEfg0dODP6k",
        "outputId": "d2c0c18a-f981-4d58-89ca-d0faad9688e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fa8b0f9f510>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_f1f74_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >rouge1</th>\n",
              "      <th class=\"col_heading level0 col1\" >rouge2</th>\n",
              "      <th class=\"col_heading level0 col2\" >rougeL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_f1f74_level0_row0\" class=\"row_heading level0 row0\" >P</th>\n",
              "      <td id=\"T_f1f74_row0_col0\" class=\"data row0 col0\" >0.0522</td>\n",
              "      <td id=\"T_f1f74_row0_col1\" class=\"data row0 col1\" >0.0031</td>\n",
              "      <td id=\"T_f1f74_row0_col2\" class=\"data row0 col2\" >0.0403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1f74_level0_row1\" class=\"row_heading level0 row1\" >R</th>\n",
              "      <td id=\"T_f1f74_row1_col0\" class=\"data row1 col0\" >0.2308</td>\n",
              "      <td id=\"T_f1f74_row1_col1\" class=\"data row1 col1\" >0.0100</td>\n",
              "      <td id=\"T_f1f74_row1_col2\" class=\"data row1 col2\" >0.1841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1f74_level0_row2\" class=\"row_heading level0 row2\" >F</th>\n",
              "      <td id=\"T_f1f74_row2_col0\" class=\"data row2 col0\" >0.0832</td>\n",
              "      <td id=\"T_f1f74_row2_col1\" class=\"data row2 col1\" >0.0048</td>\n",
              "      <td id=\"T_f1f74_row2_col2\" class=\"data row2 col2\" >0.0647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Compare each generation to the fist answer from the dataset\n",
        "nlp_rouge = nlp.load_metric('rouge')\n",
        "\n",
        "scores = nlp_rouge.compute(\n",
        "    predicted, reference,\n",
        "    rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
        "    use_agregator=True, use_stemmer=False\n",
        ")\n",
        "df = pd.DataFrame({\n",
        "    'rouge1': [scores['rouge1'].mid.precision, scores['rouge1'].mid.recall, scores['rouge1'].mid.fmeasure],\n",
        "    'rouge2': [scores['rouge2'].mid.precision, scores['rouge2'].mid.recall, scores['rouge2'].mid.fmeasure],\n",
        "    'rougeL': [scores['rougeL'].mid.precision, scores['rougeL'].mid.recall, scores['rougeL'].mid.fmeasure],\n",
        "}, index=[ 'P', 'R', 'F'])\n",
        "df.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZJ1xRl_DP6k"
      },
      "source": [
        "That's it for today! And once again, if you want to play with the model a bit more and ask it whatever question comes to mind, please feel free to head over to:\n",
        "# [**Our Live Demo!**](https://huggingface.co/qa/)  \n",
        "\n",
        "Thank you for reading!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmxDfVf8DP6k"
      },
      "source": [
        "# Appendix:\n",
        "\n",
        "Here we reproduce the ROUGE evaluation from the original [ELI5 paper](https://arxiv.org/abs/1907.09190) to be able to comparable our performance to theirs. Our generation setting leads to lower ROUGE-1 and ROUGE-2 than the state-of-the-art reported in [BART](https://arxiv.org/abs/1910.13461) (30.6 and 6.2 respectively), and higher ROUGE-L (24.3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbeMYDJNDP6k",
        "outputId": "f34706ce-0036-43be-dfa5-1805e4f4b561"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rouge1</th>        <th class=\"col_heading level0 col1\" >rouge2</th>        <th class=\"col_heading level0 col2\" >rougeL</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806level0_row0\" class=\"row_heading level0 row0\" >P</th>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row0_col0\" class=\"data row0 col0\" >0.3254</td>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row0_col1\" class=\"data row0 col1\" >0.0680</td>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row0_col2\" class=\"data row0 col2\" >0.3251</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806level0_row1\" class=\"row_heading level0 row1\" >R</th>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row1_col0\" class=\"data row1 col0\" >0.3118</td>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row1_col1\" class=\"data row1 col1\" >0.0631</td>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row1_col2\" class=\"data row1 col2\" >0.2560</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806level0_row2\" class=\"row_heading level0 row2\" >F</th>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row2_col0\" class=\"data row2 col0\" >0.2729</td>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row2_col1\" class=\"data row2 col1\" >0.0551</td>\n",
              "                        <td id=\"T_59d1df5e_acb2_11ea_bac0_cb508f4c6806row2_col2\" class=\"data row2 col2\" >0.2583</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f09776cc310>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import PorterStemmer\n",
        "from rouge import Rouge\n",
        "from spacy.lang.en import English\n",
        "from time import time\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "rouge = Rouge()\n",
        "tokenizer = English().Defaults.create_tokenizer()\n",
        "\n",
        "def compute_rouge_eli5(compare_list):\n",
        "    preds = [\" \".join([stemmer.stem(str(w))\n",
        "                       for w in tokenizer(pred)])\n",
        "             for gold, pred in compare_list]\n",
        "    golds = [\" \".join([stemmer.stem(str(w))\n",
        "                       for w in tokenizer(gold)])\n",
        "             for gold, pred in compare_list]\n",
        "    scores = rouge.get_scores(preds, golds, avg=True)\n",
        "    return scores\n",
        "\n",
        "\n",
        "compare_list = [(g, p) for p, g in zip(predicted, reference)]\n",
        "scores = compute_rouge_eli5(compare_list)\n",
        "df = pd.DataFrame({\n",
        "    'rouge1': [scores['rouge-1']['p'], scores['rouge-1']['r'], scores['rouge-1']['f']],\n",
        "    'rouge2': [scores['rouge-2']['p'], scores['rouge-2']['r'], scores['rouge-2']['f']],\n",
        "    'rougeL': [scores['rouge-l']['p'], scores['rouge-l']['r'], scores['rouge-l']['f']],\n",
        "}, index=[ 'P', 'R', 'F'])\n",
        "df.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Eli5-zero-shot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TO_nv5oFDP6a",
        "cLLOj8yeDP6f",
        "kZJ1xRl_DP6k",
        "GmxDfVf8DP6k"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}